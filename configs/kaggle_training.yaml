# Kaggle-optimized configuration for T4 x2 GPUs
# Maximizes GPU utilization with larger batches and vectorized operations

algorithm: ppo

environment:
  seed: 42

training:
  total_timesteps: 1000000
  steps_per_update: 256
  num_envs: 16  # Increased from 4 - more parallel data collection

# PPO hyperparameters - optimized for GPU throughput
ppo:
  learning_rate: 2.5e-4
  gamma: 0.99
  lambda: 0.95
  clip_param: 0.2
  entropy_coef: 0.01
  vf_coef: 0.5
  epochs: 4
  batch_size: 512  # Increased from 256 for better GPU utilization
  max_grad_norm: 0.5
  use_lr_annealing: true
  use_value_clipping: false

# Rainbow DQN - GPU-optimized settings
dqn:
  learning_rate: 6.25e-5
  gamma: 0.99
  n_step: 3
  num_atoms: 51
  v_min: -10.0
  v_max: 10.0
  # Larger buffer for better sample diversity
  buffer_size: 100000
  # CRITICAL: Large batch size for GPU utilization
  batch_size: 256  # Increased from 32 (8x larger)
  target_update_freq: 2000  # More frequent updates with larger batches
  learning_starts: 5000  # Start learning sooner
  train_freq: 1  # Train every step instead of every 4
  priority_alpha: 0.6
  priority_beta_start: 0.4
  priority_beta_frames: 100000
  use_noisy: true
  use_dueling: true
  use_double: true
  use_per: true

# RND for intrinsic motivation (PPO only)
rnd:
  enabled: true
  feature_dim: 512
  learning_rate: 1e-4
  intrinsic_reward_scale: 1.0
  update_proportion: 0.25
  intrinsic_reward_coef: 0.5

# Exploration strategies (PPO only)
exploration:
  use_epsilon_greedy: true
  epsilon_start: 0.3
  epsilon_end: 0.01
  epsilon_decay_steps: 300000
  use_ucb: true
  ucb_coefficient: 0.5
  use_adaptive_entropy: true
  target_entropy_ratio: 0.5
  entropy_lr: 0.01
  min_entropy_coef: 0.001
  max_entropy_coef: 0.1

logging:
  log_interval: 1
  save_interval: 10
  save_path: "models/kaggle_model"
  load_path: null
