# Default configuration for PPO training on Maze environment
# This file can be passed using --config path/to/train_config.yaml
# Command-line arguments will override settings in this file.

# --- Core Training Parameters ---
env_id: "Maze-v0"
epochs: 50
steps_per_epoch: 4000
save_freq: 10         # Save weights every N epochs
render: false         # Render environment during training
seed: 42
save_dir: "checkpoints" # Directory to save checkpoints and logs

# --- PPO Agent Hyperparameters ---
gamma: 0.99           # Discount factor
lam: 0.97             # GAE lambda parameter
clip_ratio: 0.2       # PPO clipping ratio
policy_learning_rate: 0.0003
value_function_learning_rate: 0.001
train_policy_iterations: 80 # Policy training iterations per epoch
train_value_iterations: 80  # Value function training iterations per epoch
target_kl: 0.01       # Target KL divergence for early stopping policy training

# --- Network Parameters ---
# These define the architecture of the policy and value networks
conv_filters: 32      # Number of filters in convolutional layers
conv_kernel_size: 3   # Kernel size for convolutional layers
dense_units: 128      # Number of units in dense layers
