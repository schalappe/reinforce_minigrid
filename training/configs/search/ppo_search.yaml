base_config:
  agent:
    agent_type: PPO
    clip_range: 0.2
    discount_factor: 0.99
    embedding_size: 128
    learning_rate: 0.0003
    gae_lambda: 0.95
    entropy_coef: 0.01
    value_coef: 0.5
    max_grad_norm: 0.5
    use_adaptive_kl: true
    lr_schedule_enabled: true
  environment:
    use_image_obs: true
  trainer:
    trainer_type: PPOTrainer
    n_steps: 2048
    n_epochs: 10
    batch_size: 64
    max_total_steps: 2000
    max_steps_per_episode: 500
    eval_frequency: 100
    num_eval_episodes: 5
    log_frequency: 10
    save_frequency: 500
    save_dir: outputs/models/ppo_search_temp

hyperparameters:
  agent.learning_rate:
    type: float
    low: 1.0e-5
    high: 0.001
    log_scale: true
  agent.entropy_coef:
    type: float
    low: 0.0
    high: 0.05
  agent.value_coef:
    type: float
    low: 0.3
    high: 0.7
  agent.discount_factor:
    type: categorical
    values:
      - 0.95
      - 0.99
      - 0.995
  agent.embedding_size:
    type: int
    low: 64
    high: 256
    step: 64
  agent.gae_lambda:
    type: float
    low: 0.9
    high: 0.99
  agent.clip_range:
    type: float
    low: 0.1
    high: 0.3
  agent.use_adaptive_kl:
    type: bool
  agent.lr_schedule_enabled:
    type: bool
  agent.max_grad_norm:
    type: float
    low: 0.3
    high: 1.0
  trainer.n_steps:
    type: int
    low: 512
    high: 2048
    step: 512
  trainer.n_epochs:
    type: int
    low: 5
    high: 20
  trainer.batch_size:
    type: categorical
    values:
      - 32
      - 64
      - 128
