# training/configs/ppo_search.yaml
base_config:
  agent:
    agent_type: PPO
    action_space: 7
    embedding_size: 128
    learning_rate: 0.0003
    discount_factor: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    entropy_coef: 0.01
    value_coef: 0.5
    max_grad_norm: 0.5
  environment:
    use_image_obs: true
  trainer:
    trainer_type: PPOTrainer
    n_steps: 2048
    n_epochs: 10
    batch_size: 64
    max_total_steps: 5000
    max_steps_per_episode: 100
    eval_frequency: 5
    num_eval_episodes: 5
    log_frequency: 10
    log_env_image_frequency: 10000
    save_frequency: 100
    save_dir: outputs/models/ppo_search_temp

hyperparameters:
  agent.learning_rate:
    type: float
    low: 1.0e-5
    high: 0.001
    log_scale: true
  agent.entropy_coef:
    type: float
    low: 0.0
    high: 0.05
  agent.value_coef:
    type: float
    low: 0.3
    high: 0.7
  agent.discount_factor:
    type: categorical
    values:
      - 0.95
      - 0.99
      - 0.995
  agent.embedding_size:
    type: int
    low: 64
    high: 256
    step: 64
  agent.gae_lambda:
    type: float
    low: 0.9
    high: 0.99
  agent.clip_range:
    type: float
    low: 0.1
    high: 0.3
  trainer.n_steps:
    type: int
    low: 512
    high: 4096
    step: 512
  trainer.n_epochs:
    type: int
    low: 5
    high: 20
  trainer.batch_size:
    type: categorical
    values:
      - 32
      - 64
      - 128
